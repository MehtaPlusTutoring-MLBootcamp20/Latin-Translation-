{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Latin translation final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "40_IfWZi0XL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEB7A_8L1aKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        " \n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8eJmyDlumRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zXrU9EBqZZI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c1352a0a-23b8-4aa8-b539-aad4756cd370"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DESn40jkItAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load dataset\n",
        "filename = \"/content/drive/My Drive/Latin Translation ML project/Dataset1.csv\"\n",
        "doc=load_doc(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXgciW4XuPrl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "e9ddf9a5-2dd0-44c4-d887-0e5a30bd9603"
      },
      "source": [
        "dataset_original = pd.read_csv(filename,converters={i: str for i in range(0, 2)})\n",
        "dataset_original"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>great</th>\n",
              "      <th>māgnus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>great</td>\n",
              "      <td>māgna</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>great</td>\n",
              "      <td>māgnum</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>his own</td>\n",
              "      <td>suus</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>her own</td>\n",
              "      <td>sua</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>its own</td>\n",
              "      <td>suum</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18232</th>\n",
              "      <td>you</td>\n",
              "      <td>vōbīs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18233</th>\n",
              "      <td>you</td>\n",
              "      <td>vōs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18234</th>\n",
              "      <td>freeborn children</td>\n",
              "      <td>līberī</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18235</th>\n",
              "      <td>swiftly</td>\n",
              "      <td>citō</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18236</th>\n",
              "      <td>from here</td>\n",
              "      <td>hinc</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18237 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   great  māgnus\n",
              "0                  great   māgna\n",
              "1                  great  māgnum\n",
              "2                his own    suus\n",
              "3                her own     sua\n",
              "4                its own    suum\n",
              "...                  ...     ...\n",
              "18232                you   vōbīs\n",
              "18233                you     vōs\n",
              "18234  freeborn children  līberī\n",
              "18235            swiftly    citō\n",
              "18236          from here    hinc\n",
              "\n",
              "[18237 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpT9w6uDuTA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_pairs= dataset_original.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCLTn46AqSSp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "955e7cf0-81dd-4592-ce4c-adc99786d9b2"
      },
      "source": [
        "\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-latin.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-latin.pkl\n",
            "[great] => [māgna]\n",
            "[great] => [māgnum]\n",
            "[his own] => [suus]\n",
            "[her own] => [sua]\n",
            "[its own] => [suum]\n",
            "[other] => [alius]\n",
            "[another] => [alius]\n",
            "[other] => [alia]\n",
            "[another] => [alia]\n",
            "[other] => [aliud]\n",
            "[another] => [aliud]\n",
            "[much] => [multus]\n",
            "[many] => [multus]\n",
            "[much] => [multa]\n",
            "[many] => [multa]\n",
            "[much] => [multum]\n",
            "[many] => [multum]\n",
            "[your] => [tuus]\n",
            "[your] => [tua]\n",
            "[your] => [tuum]\n",
            "[not any] => [nūllus]\n",
            "[no one] => [nūllus]\n",
            "[not any] => [nūlla]\n",
            "[no one] => [nūlla]\n",
            "[not any] => [nūllum]\n",
            "[no one] => [nūllum]\n",
            "[one] => [ūnus]\n",
            "[one] => [ūna]\n",
            "[one] => [ūnum]\n",
            "[good] => [bonus]\n",
            "[good] => [bona]\n",
            "[good] => [bonum]\n",
            "[whole] => [tōtus]\n",
            "[entire] => [tōtus]\n",
            "[whole] => [tōta]\n",
            "[entire] => [tōta]\n",
            "[whole] => [tōtum]\n",
            "[entire] => [tōtum]\n",
            "[first] => [prīmus]\n",
            "[first] => [prīma]\n",
            "[first] => [prīmum]\n",
            "[situated above] => [superus]\n",
            "[upper] => [superus]\n",
            "[situated above] => [supera]\n",
            "[upper] => [supera]\n",
            "[situated above] => [superum]\n",
            "[upper] => [superum]\n",
            "[so great] => [tantus]\n",
            "[so much] => [tantus]\n",
            "[so great] => [tanta]\n",
            "[so much] => [tanta]\n",
            "[so great] => [tantum]\n",
            "[so much] => [tantum]\n",
            "[at such a price] => [tantī]\n",
            "[of such worth] => [tantī]\n",
            "[you] => [vōs]\n",
            "[wretched] => [miser]\n",
            "[pitiable] => [miser]\n",
            "[wretched] => [misera]\n",
            "[pitiable] => [misera]\n",
            "[wretched] => [miserum]\n",
            "[pitiable] => [miserum]\n",
            "[new] => [novus]\n",
            "[new] => [nova]\n",
            "[new] => [novum]\n",
            "[long] => [longus]\n",
            "[far] => [longus]\n",
            "[long] => [longa]\n",
            "[far] => [longa]\n",
            "[long] => [longum]\n",
            "[far] => [longum]\n",
            "[small] => [parvus]\n",
            "[small] => [parva]\n",
            "[small] => [parvum]\n",
            "[other of two] => [alter]\n",
            "[other of two] => [altera]\n",
            "[other of two] => [alterum]\n",
            "[high] => [altus]\n",
            "[lofty] => [altus]\n",
            "[deep] => [altus]\n",
            "[high] => [alta]\n",
            "[lofty] => [alta]\n",
            "[deep] => [alta]\n",
            "[high] => [altum]\n",
            "[lofty] => [altum]\n",
            "[deep] => [altum]\n",
            "[middle] => [medius]\n",
            "[central] => [medius]\n",
            "[middle] => [media]\n",
            "[central] => [media]\n",
            "[middle] => [medium]\n",
            "[central] => [medium]\n",
            "[only] => [sōlus]\n",
            "[alone] => [sōlus]\n",
            "[only] => [sōla]\n",
            "[alone] => [sōla]\n",
            "[only] => [sōlum]\n",
            "[alone] => [sōlum]\n",
            "[any] => [ūllus]\n",
            "[anyone] => [ūllus]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-lMl5Xn1_GE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ae7ac6c9-5616-4956-daa5-ec1ecac50790"
      },
      "source": [
        "clean_pairs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['great', 'māgna'],\n",
              "       ['great', 'māgnum'],\n",
              "       ['his own', 'suus'],\n",
              "       ...,\n",
              "       ['freeborn children', 'līberī'],\n",
              "       ['swiftly', 'citō'],\n",
              "       ['from here', 'hinc']], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0HgFe3ptGZE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6d4b1d2d-3da0-4373-dfbd-ba75df8acb9c"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        " \n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        " \n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-latin.pkl')\n",
        "# random shuffle\n",
        "shuffle(raw_dataset)\n",
        "# reduce dataset size\n",
        "n_sentences = 13000\n",
        "dataset = raw_dataset[:n_sentences]\n",
        "# split into train/test\n",
        "train, test = dataset[:11000], dataset[11000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-latin-both.pkl')\n",
        "save_clean_data(train, 'english-latin-train.pkl')\n",
        "save_clean_data(test, 'english-latin-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-latin-both.pkl\n",
            "Saved: english-latin-train.pkl\n",
            "Saved: english-latin-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce8CYu18ic-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f298bfca-a39d-42ed-aa1a-1b8d3e6394f7"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcRCxTDdtSWV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "348aa94e-215c-495f-a4ca-967e9423e092"
      },
      "source": [
        "\n",
        " \n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        " \n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        " \n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        " \n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        " \n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        " \n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        " \n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-latin-both.pkl')\n",
        "train = load_clean_sentences('english-latin-train.pkl')\n",
        "test = load_clean_sentences('english-latin-test.pkl')\n",
        " \n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "lat_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "lat_vocab_size = len(lat_tokenizer.word_index) + 1\n",
        "lat_length = max_length(dataset[:, 1])\n",
        "print('Latin Vocabulary Size: %d' % lat_vocab_size)\n",
        "print('Latin Max Length: %d' % (lat_length))\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 4363\n",
            "English Max Length: 30\n",
            "Latin Vocabulary Size: 7657\n",
            "Latin Max Length: 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3kyKdqw8gcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare training data\n",
        "trainX = encode_sequences(lat_tokenizer, lat_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(lat_tokenizer, lat_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyYzWn7M8mUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6288e842-556e-4bcb-9dd8-ea62a84cb8df"
      },
      "source": [
        "# define model\n",
        "model = define_model(lat_vocab_size, eng_vocab_size, lat_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='Latinmodel2.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'Latinmodel2.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=120, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 19, 256)           1960192   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 30, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 30, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 30, 4363)          1121291   \n",
            "=================================================================\n",
            "Total params: 4,132,107\n",
            "Trainable params: 4,132,107\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 11000 samples, validate on 2000 samples\n",
            "Epoch 1/120\n",
            " - 151s - loss: 1.7040 - acc: 0.8819 - val_loss: 0.9970 - val_acc: 0.8937\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.99696, saving model to Latinmodel2.h5\n",
            "Epoch 2/120\n",
            " - 151s - loss: 0.8913 - acc: 0.8949 - val_loss: 0.8343 - val_acc: 0.8959\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.99696 to 0.83432, saving model to Latinmodel2.h5\n",
            "Epoch 3/120\n",
            " - 150s - loss: 0.7935 - acc: 0.8978 - val_loss: 0.8005 - val_acc: 0.8966\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.83432 to 0.80047, saving model to Latinmodel2.h5\n",
            "Epoch 4/120\n",
            " - 151s - loss: 0.7587 - acc: 0.9002 - val_loss: 0.7812 - val_acc: 0.8989\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.80047 to 0.78123, saving model to Latinmodel2.h5\n",
            "Epoch 5/120\n",
            " - 150s - loss: 0.7258 - acc: 0.9018 - val_loss: 0.7657 - val_acc: 0.8990\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.78123 to 0.76566, saving model to Latinmodel2.h5\n",
            "Epoch 6/120\n",
            " - 150s - loss: 0.7025 - acc: 0.9026 - val_loss: 0.7600 - val_acc: 0.8990\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.76566 to 0.76003, saving model to Latinmodel2.h5\n",
            "Epoch 7/120\n",
            " - 150s - loss: 0.6895 - acc: 0.9025 - val_loss: 0.7543 - val_acc: 0.8989\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.76003 to 0.75426, saving model to Latinmodel2.h5\n",
            "Epoch 8/120\n",
            " - 150s - loss: 0.6757 - acc: 0.9028 - val_loss: 0.7483 - val_acc: 0.8984\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.75426 to 0.74830, saving model to Latinmodel2.h5\n",
            "Epoch 9/120\n",
            " - 150s - loss: 0.6633 - acc: 0.9032 - val_loss: 0.7449 - val_acc: 0.8984\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.74830 to 0.74486, saving model to Latinmodel2.h5\n",
            "Epoch 10/120\n",
            " - 150s - loss: 0.6530 - acc: 0.9034 - val_loss: 0.7420 - val_acc: 0.8980\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.74486 to 0.74199, saving model to Latinmodel2.h5\n",
            "Epoch 11/120\n",
            " - 151s - loss: 0.6428 - acc: 0.9037 - val_loss: 0.7425 - val_acc: 0.8988\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.74199\n",
            "Epoch 12/120\n",
            " - 150s - loss: 0.6315 - acc: 0.9041 - val_loss: 0.7300 - val_acc: 0.8986\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.74199 to 0.72998, saving model to Latinmodel2.h5\n",
            "Epoch 13/120\n",
            " - 150s - loss: 0.6186 - acc: 0.9045 - val_loss: 0.7288 - val_acc: 0.8990\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.72998 to 0.72875, saving model to Latinmodel2.h5\n",
            "Epoch 14/120\n",
            " - 150s - loss: 0.6065 - acc: 0.9051 - val_loss: 0.7284 - val_acc: 0.8993\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.72875 to 0.72835, saving model to Latinmodel2.h5\n",
            "Epoch 15/120\n",
            " - 150s - loss: 0.5932 - acc: 0.9056 - val_loss: 0.7174 - val_acc: 0.8998\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.72835 to 0.71740, saving model to Latinmodel2.h5\n",
            "Epoch 16/120\n",
            " - 150s - loss: 0.5787 - acc: 0.9061 - val_loss: 0.7115 - val_acc: 0.8995\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.71740 to 0.71146, saving model to Latinmodel2.h5\n",
            "Epoch 17/120\n",
            " - 151s - loss: 0.5631 - acc: 0.9071 - val_loss: 0.7007 - val_acc: 0.8992\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.71146 to 0.70074, saving model to Latinmodel2.h5\n",
            "Epoch 18/120\n",
            " - 150s - loss: 0.5488 - acc: 0.9079 - val_loss: 0.6946 - val_acc: 0.9001\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.70074 to 0.69461, saving model to Latinmodel2.h5\n",
            "Epoch 19/120\n",
            " - 150s - loss: 0.5306 - acc: 0.9089 - val_loss: 0.6888 - val_acc: 0.9013\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.69461 to 0.68877, saving model to Latinmodel2.h5\n",
            "Epoch 20/120\n",
            " - 150s - loss: 0.5123 - acc: 0.9102 - val_loss: 0.6811 - val_acc: 0.9019\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.68877 to 0.68111, saving model to Latinmodel2.h5\n",
            "Epoch 21/120\n",
            " - 150s - loss: 0.4943 - acc: 0.9115 - val_loss: 0.6751 - val_acc: 0.9031\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.68111 to 0.67510, saving model to Latinmodel2.h5\n",
            "Epoch 22/120\n",
            " - 150s - loss: 0.4764 - acc: 0.9128 - val_loss: 0.6700 - val_acc: 0.9035\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.67510 to 0.66999, saving model to Latinmodel2.h5\n",
            "Epoch 23/120\n",
            " - 151s - loss: 0.4593 - acc: 0.9142 - val_loss: 0.6611 - val_acc: 0.9034\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.66999 to 0.66111, saving model to Latinmodel2.h5\n",
            "Epoch 24/120\n",
            " - 150s - loss: 0.4421 - acc: 0.9156 - val_loss: 0.6576 - val_acc: 0.9042\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.66111 to 0.65757, saving model to Latinmodel2.h5\n",
            "Epoch 25/120\n",
            " - 150s - loss: 0.4277 - acc: 0.9173 - val_loss: 0.6466 - val_acc: 0.9046\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.65757 to 0.64661, saving model to Latinmodel2.h5\n",
            "Epoch 26/120\n",
            " - 150s - loss: 0.4101 - acc: 0.9191 - val_loss: 0.6439 - val_acc: 0.9057\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.64661 to 0.64385, saving model to Latinmodel2.h5\n",
            "Epoch 27/120\n",
            " - 151s - loss: 0.3936 - acc: 0.9207 - val_loss: 0.6415 - val_acc: 0.9063\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.64385 to 0.64153, saving model to Latinmodel2.h5\n",
            "Epoch 28/120\n",
            " - 150s - loss: 0.3792 - acc: 0.9223 - val_loss: 0.6344 - val_acc: 0.9072\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.64153 to 0.63441, saving model to Latinmodel2.h5\n",
            "Epoch 29/120\n",
            " - 151s - loss: 0.3648 - acc: 0.9241 - val_loss: 0.6303 - val_acc: 0.9075\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.63441 to 0.63032, saving model to Latinmodel2.h5\n",
            "Epoch 30/120\n",
            " - 150s - loss: 0.3502 - acc: 0.9262 - val_loss: 0.6255 - val_acc: 0.9082\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.63032 to 0.62553, saving model to Latinmodel2.h5\n",
            "Epoch 31/120\n",
            " - 151s - loss: 0.3373 - acc: 0.9277 - val_loss: 0.6245 - val_acc: 0.9093\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.62553 to 0.62453, saving model to Latinmodel2.h5\n",
            "Epoch 32/120\n",
            " - 150s - loss: 0.3238 - acc: 0.9291 - val_loss: 0.6216 - val_acc: 0.9094\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.62453 to 0.62158, saving model to Latinmodel2.h5\n",
            "Epoch 33/120\n",
            " - 150s - loss: 0.3118 - acc: 0.9310 - val_loss: 0.6187 - val_acc: 0.9109\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.62158 to 0.61871, saving model to Latinmodel2.h5\n",
            "Epoch 34/120\n",
            " - 149s - loss: 0.3004 - acc: 0.9327 - val_loss: 0.6167 - val_acc: 0.9117\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.61871 to 0.61665, saving model to Latinmodel2.h5\n",
            "Epoch 35/120\n",
            " - 149s - loss: 0.2898 - acc: 0.9340 - val_loss: 0.6172 - val_acc: 0.9119\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.61665\n",
            "Epoch 36/120\n",
            " - 148s - loss: 0.2796 - acc: 0.9353 - val_loss: 0.6122 - val_acc: 0.9122\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.61665 to 0.61222, saving model to Latinmodel2.h5\n",
            "Epoch 37/120\n",
            " - 149s - loss: 0.2688 - acc: 0.9372 - val_loss: 0.6111 - val_acc: 0.9140\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.61222 to 0.61114, saving model to Latinmodel2.h5\n",
            "Epoch 38/120\n",
            " - 149s - loss: 0.2595 - acc: 0.9385 - val_loss: 0.6054 - val_acc: 0.9132\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.61114 to 0.60542, saving model to Latinmodel2.h5\n",
            "Epoch 39/120\n",
            " - 149s - loss: 0.2506 - acc: 0.9400 - val_loss: 0.6067 - val_acc: 0.9140\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.60542\n",
            "Epoch 40/120\n",
            " - 149s - loss: 0.2433 - acc: 0.9408 - val_loss: 0.6026 - val_acc: 0.9147\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.60542 to 0.60258, saving model to Latinmodel2.h5\n",
            "Epoch 41/120\n",
            " - 149s - loss: 0.2345 - acc: 0.9425 - val_loss: 0.5998 - val_acc: 0.9152\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.60258 to 0.59979, saving model to Latinmodel2.h5\n",
            "Epoch 42/120\n",
            " - 149s - loss: 0.2255 - acc: 0.9442 - val_loss: 0.6021 - val_acc: 0.9150\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.59979\n",
            "Epoch 43/120\n",
            " - 149s - loss: 0.2172 - acc: 0.9453 - val_loss: 0.6019 - val_acc: 0.9162\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.59979\n",
            "Epoch 44/120\n",
            " - 149s - loss: 0.2100 - acc: 0.9467 - val_loss: 0.6030 - val_acc: 0.9169\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.59979\n",
            "Epoch 45/120\n",
            " - 149s - loss: 0.2033 - acc: 0.9475 - val_loss: 0.5997 - val_acc: 0.9177\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.59979 to 0.59974, saving model to Latinmodel2.h5\n",
            "Epoch 46/120\n",
            " - 149s - loss: 0.1967 - acc: 0.9491 - val_loss: 0.5996 - val_acc: 0.9171\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.59974 to 0.59965, saving model to Latinmodel2.h5\n",
            "Epoch 47/120\n",
            " - 149s - loss: 0.1901 - acc: 0.9500 - val_loss: 0.5979 - val_acc: 0.9179\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.59965 to 0.59786, saving model to Latinmodel2.h5\n",
            "Epoch 48/120\n",
            " - 149s - loss: 0.1844 - acc: 0.9510 - val_loss: 0.6013 - val_acc: 0.9173\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.59786\n",
            "Epoch 49/120\n",
            " - 149s - loss: 0.1781 - acc: 0.9524 - val_loss: 0.5977 - val_acc: 0.9183\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.59786 to 0.59765, saving model to Latinmodel2.h5\n",
            "Epoch 50/120\n",
            " - 149s - loss: 0.1742 - acc: 0.9527 - val_loss: 0.5996 - val_acc: 0.9192\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.59765\n",
            "Epoch 51/120\n",
            " - 149s - loss: 0.1678 - acc: 0.9540 - val_loss: 0.6017 - val_acc: 0.9197\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.59765\n",
            "Epoch 52/120\n",
            " - 149s - loss: 0.1631 - acc: 0.9549 - val_loss: 0.6004 - val_acc: 0.9192\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.59765\n",
            "Epoch 53/120\n",
            " - 150s - loss: 0.1575 - acc: 0.9560 - val_loss: 0.6016 - val_acc: 0.9198\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.59765\n",
            "Epoch 54/120\n",
            " - 149s - loss: 0.1536 - acc: 0.9568 - val_loss: 0.6035 - val_acc: 0.9182\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.59765\n",
            "Epoch 55/120\n",
            " - 149s - loss: 0.1492 - acc: 0.9574 - val_loss: 0.5996 - val_acc: 0.9207\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.59765\n",
            "Epoch 56/120\n",
            " - 149s - loss: 0.1443 - acc: 0.9586 - val_loss: 0.5992 - val_acc: 0.9216\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.59765\n",
            "Epoch 57/120\n",
            " - 149s - loss: 0.1405 - acc: 0.9591 - val_loss: 0.6005 - val_acc: 0.9217\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.59765\n",
            "Epoch 58/120\n",
            " - 149s - loss: 0.1371 - acc: 0.9598 - val_loss: 0.6022 - val_acc: 0.9217\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.59765\n",
            "Epoch 59/120\n",
            " - 149s - loss: 0.1332 - acc: 0.9602 - val_loss: 0.6056 - val_acc: 0.9214\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.59765\n",
            "Epoch 60/120\n",
            " - 149s - loss: 0.1306 - acc: 0.9607 - val_loss: 0.6045 - val_acc: 0.9220\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.59765\n",
            "Epoch 61/120\n",
            " - 149s - loss: 0.1265 - acc: 0.9618 - val_loss: 0.6046 - val_acc: 0.9220\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.59765\n",
            "Epoch 62/120\n",
            " - 149s - loss: 0.1235 - acc: 0.9623 - val_loss: 0.6046 - val_acc: 0.9230\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.59765\n",
            "Epoch 63/120\n",
            " - 149s - loss: 0.1216 - acc: 0.9625 - val_loss: 0.6064 - val_acc: 0.9227\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.59765\n",
            "Epoch 64/120\n",
            " - 149s - loss: 0.1165 - acc: 0.9641 - val_loss: 0.6086 - val_acc: 0.9227\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.59765\n",
            "Epoch 65/120\n",
            " - 148s - loss: 0.1145 - acc: 0.9641 - val_loss: 0.6092 - val_acc: 0.9232\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.59765\n",
            "Epoch 66/120\n",
            " - 148s - loss: 0.1118 - acc: 0.9647 - val_loss: 0.6073 - val_acc: 0.9239\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.59765\n",
            "Epoch 67/120\n",
            " - 148s - loss: 0.1092 - acc: 0.9653 - val_loss: 0.6132 - val_acc: 0.9242\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.59765\n",
            "Epoch 68/120\n",
            " - 149s - loss: 0.1064 - acc: 0.9658 - val_loss: 0.6109 - val_acc: 0.9242\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.59765\n",
            "Epoch 69/120\n",
            " - 150s - loss: 0.1054 - acc: 0.9660 - val_loss: 0.6130 - val_acc: 0.9231\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.59765\n",
            "Epoch 70/120\n",
            " - 150s - loss: 0.1034 - acc: 0.9661 - val_loss: 0.6103 - val_acc: 0.9257\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.59765\n",
            "Epoch 71/120\n",
            " - 150s - loss: 0.0992 - acc: 0.9675 - val_loss: 0.6124 - val_acc: 0.9244\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.59765\n",
            "Epoch 72/120\n",
            " - 149s - loss: 0.0962 - acc: 0.9684 - val_loss: 0.6151 - val_acc: 0.9255\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.59765\n",
            "Epoch 73/120\n",
            " - 150s - loss: 0.0948 - acc: 0.9687 - val_loss: 0.6175 - val_acc: 0.9251\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.59765\n",
            "Epoch 74/120\n",
            " - 150s - loss: 0.0952 - acc: 0.9681 - val_loss: 0.6190 - val_acc: 0.9251\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.59765\n",
            "Epoch 75/120\n",
            " - 149s - loss: 0.0939 - acc: 0.9683 - val_loss: 0.6194 - val_acc: 0.9245\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.59765\n",
            "Epoch 76/120\n",
            " - 148s - loss: 0.0919 - acc: 0.9688 - val_loss: 0.6202 - val_acc: 0.9258\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.59765\n",
            "Epoch 77/120\n",
            " - 149s - loss: 0.0896 - acc: 0.9692 - val_loss: 0.6198 - val_acc: 0.9260\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.59765\n",
            "Epoch 78/120\n",
            " - 149s - loss: 0.0874 - acc: 0.9700 - val_loss: 0.6194 - val_acc: 0.9258\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.59765\n",
            "Epoch 79/120\n",
            " - 149s - loss: 0.0857 - acc: 0.9704 - val_loss: 0.6213 - val_acc: 0.9255\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.59765\n",
            "Epoch 80/120\n",
            " - 149s - loss: 0.0839 - acc: 0.9709 - val_loss: 0.6237 - val_acc: 0.9257\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.59765\n",
            "Epoch 81/120\n",
            " - 149s - loss: 0.0822 - acc: 0.9713 - val_loss: 0.6257 - val_acc: 0.9251\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.59765\n",
            "Epoch 82/120\n",
            " - 149s - loss: 0.0817 - acc: 0.9711 - val_loss: 0.6279 - val_acc: 0.9259\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.59765\n",
            "Epoch 83/120\n",
            " - 150s - loss: 0.0800 - acc: 0.9715 - val_loss: 0.6290 - val_acc: 0.9266\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.59765\n",
            "Epoch 84/120\n",
            " - 149s - loss: 0.0791 - acc: 0.9715 - val_loss: 0.6292 - val_acc: 0.9246\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.59765\n",
            "Epoch 85/120\n",
            " - 150s - loss: 0.0790 - acc: 0.9715 - val_loss: 0.6308 - val_acc: 0.9258\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.59765\n",
            "Epoch 86/120\n",
            " - 150s - loss: 0.0765 - acc: 0.9725 - val_loss: 0.6311 - val_acc: 0.9266\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.59765\n",
            "Epoch 87/120\n",
            " - 150s - loss: 0.0758 - acc: 0.9727 - val_loss: 0.6342 - val_acc: 0.9268\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.59765\n",
            "Epoch 88/120\n",
            " - 149s - loss: 0.0740 - acc: 0.9728 - val_loss: 0.6332 - val_acc: 0.9262\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.59765\n",
            "Epoch 89/120\n",
            " - 150s - loss: 0.0714 - acc: 0.9738 - val_loss: 0.6342 - val_acc: 0.9262\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.59765\n",
            "Epoch 90/120\n",
            " - 150s - loss: 0.0701 - acc: 0.9741 - val_loss: 0.6366 - val_acc: 0.9257\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.59765\n",
            "Epoch 91/120\n",
            " - 150s - loss: 0.0702 - acc: 0.9741 - val_loss: 0.6376 - val_acc: 0.9263\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.59765\n",
            "Epoch 92/120\n",
            " - 149s - loss: 0.0703 - acc: 0.9738 - val_loss: 0.6382 - val_acc: 0.9270\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.59765\n",
            "Epoch 93/120\n",
            " - 149s - loss: 0.0698 - acc: 0.9739 - val_loss: 0.6381 - val_acc: 0.9268\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.59765\n",
            "Epoch 94/120\n",
            " - 149s - loss: 0.0695 - acc: 0.9737 - val_loss: 0.6381 - val_acc: 0.9262\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.59765\n",
            "Epoch 95/120\n",
            " - 149s - loss: 0.0685 - acc: 0.9740 - val_loss: 0.6424 - val_acc: 0.9259\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.59765\n",
            "Epoch 96/120\n",
            " - 149s - loss: 0.0685 - acc: 0.9739 - val_loss: 0.6387 - val_acc: 0.9268\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.59765\n",
            "Epoch 97/120\n",
            " - 149s - loss: 0.0679 - acc: 0.9741 - val_loss: 0.6426 - val_acc: 0.9266\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.59765\n",
            "Epoch 98/120\n",
            " - 150s - loss: 0.0659 - acc: 0.9749 - val_loss: 0.6446 - val_acc: 0.9266\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.59765\n",
            "Epoch 99/120\n",
            " - 149s - loss: 0.0643 - acc: 0.9753 - val_loss: 0.6438 - val_acc: 0.9265\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.59765\n",
            "Epoch 100/120\n",
            " - 150s - loss: 0.0624 - acc: 0.9757 - val_loss: 0.6466 - val_acc: 0.9273\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.59765\n",
            "Epoch 101/120\n",
            " - 149s - loss: 0.0618 - acc: 0.9760 - val_loss: 0.6474 - val_acc: 0.9277\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.59765\n",
            "Epoch 102/120\n",
            " - 150s - loss: 0.0616 - acc: 0.9758 - val_loss: 0.6459 - val_acc: 0.9280\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.59765\n",
            "Epoch 103/120\n",
            " - 149s - loss: 0.0609 - acc: 0.9761 - val_loss: 0.6495 - val_acc: 0.9262\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.59765\n",
            "Epoch 104/120\n",
            " - 149s - loss: 0.0611 - acc: 0.9758 - val_loss: 0.6518 - val_acc: 0.9270\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.59765\n",
            "Epoch 105/120\n",
            " - 149s - loss: 0.0612 - acc: 0.9759 - val_loss: 0.6522 - val_acc: 0.9276\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.59765\n",
            "Epoch 106/120\n",
            " - 150s - loss: 0.0612 - acc: 0.9758 - val_loss: 0.6516 - val_acc: 0.9274\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.59765\n",
            "Epoch 107/120\n",
            " - 149s - loss: 0.0609 - acc: 0.9760 - val_loss: 0.6536 - val_acc: 0.9268\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.59765\n",
            "Epoch 108/120\n",
            " - 150s - loss: 0.0597 - acc: 0.9765 - val_loss: 0.6552 - val_acc: 0.9269\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.59765\n",
            "Epoch 109/120\n",
            " - 150s - loss: 0.0598 - acc: 0.9760 - val_loss: 0.6577 - val_acc: 0.9259\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.59765\n",
            "Epoch 110/120\n",
            " - 150s - loss: 0.0583 - acc: 0.9767 - val_loss: 0.6596 - val_acc: 0.9255\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.59765\n",
            "Epoch 111/120\n",
            " - 149s - loss: 0.0573 - acc: 0.9767 - val_loss: 0.6592 - val_acc: 0.9265\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.59765\n",
            "Epoch 112/120\n",
            " - 149s - loss: 0.0564 - acc: 0.9771 - val_loss: 0.6559 - val_acc: 0.9265\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.59765\n",
            "Epoch 113/120\n",
            " - 149s - loss: 0.0558 - acc: 0.9772 - val_loss: 0.6599 - val_acc: 0.9262\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.59765\n",
            "Epoch 114/120\n",
            " - 150s - loss: 0.0545 - acc: 0.9776 - val_loss: 0.6579 - val_acc: 0.9276\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.59765\n",
            "Epoch 115/120\n",
            " - 149s - loss: 0.0550 - acc: 0.9775 - val_loss: 0.6603 - val_acc: 0.9272\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.59765\n",
            "Epoch 116/120\n",
            " - 150s - loss: 0.0559 - acc: 0.9771 - val_loss: 0.6639 - val_acc: 0.9275\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.59765\n",
            "Epoch 117/120\n",
            " - 150s - loss: 0.0562 - acc: 0.9770 - val_loss: 0.6640 - val_acc: 0.9272\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.59765\n",
            "Epoch 118/120\n",
            " - 150s - loss: 0.0559 - acc: 0.9770 - val_loss: 0.6651 - val_acc: 0.9257\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.59765\n",
            "Epoch 119/120\n",
            " - 150s - loss: 0.0551 - acc: 0.9773 - val_loss: 0.6634 - val_acc: 0.9273\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.59765\n",
            "Epoch 120/120\n",
            " - 150s - loss: 0.0555 - acc: 0.9774 - val_loss: 0.6643 - val_acc: 0.9272\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.59765\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f9d0ddc9fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvchE-M_380y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp Latinmodel2.h5 drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p9Y230X1cDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "tk= Tokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7gIx03E1zkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s-y2gLk4CTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = '/content/Latinmodel2.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOsFMkAj4En4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7f3e9e44-655f-485c-8efa-9a1caa800fe6"
      },
      "source": [
        "model = load_model(model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFf5daWzFwlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9NENJzMTdPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_rk59jJTC3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds_text = []\n",
        "for i in preds:\n",
        "       temp = []\n",
        "       for j in range(len(i)):\n",
        "            t = word_for_id(i[j], eng_tokenizer)\n",
        "            if j > 0: \n",
        "                if (t == word_for_id(i[j-1], eng_tokenizer)) or (t == None):\n",
        "                     temp.append('')\n",
        "                else:\n",
        "                     temp.append(t)\n",
        "            else:\n",
        "                   if(t == None):\n",
        "                          temp.append('')\n",
        "                   else:\n",
        "                          temp.append(t) \n",
        "\n",
        "       preds_text.append(' '.join(temp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7lbd7hSTLcB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c9c470f-e7dd-4b12-8e4c-b8d0a233cce1"
      },
      "source": [
        "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})\n",
        "pred_df.sample(60)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>523</th>\n",
              "      <td>Painter unknown</td>\n",
              "      <td>unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1374</th>\n",
              "      <td>The following age will be amazed</td>\n",
              "      <td>it age  be amazed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>744</th>\n",
              "      <td>set forth</td>\n",
              "      <td>go</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>robe</td>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1572</th>\n",
              "      <td>fire</td>\n",
              "      <td>of fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>which was to be done</td>\n",
              "      <td>which was to be done</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>this</td>\n",
              "      <td>this</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1698</th>\n",
              "      <td>of sea</td>\n",
              "      <td>of plain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>to forms</td>\n",
              "      <td>forms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1919</th>\n",
              "      <td>chances</td>\n",
              "      <td>o</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>650</th>\n",
              "      <td>extreme</td>\n",
              "      <td>situated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1377</th>\n",
              "      <td>Wonderful to behold</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630</th>\n",
              "      <td>The source and origin</td>\n",
              "      <td>the is of in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>684</th>\n",
              "      <td>False in one thing, false in all</td>\n",
              "      <td>there is  no  in no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1384</th>\n",
              "      <td>My conscience means more to me than all speech</td>\n",
              "      <td>and is case to  not  than will in  glory      ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1932</th>\n",
              "      <td>hopes</td>\n",
              "      <td>to hopes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>717</th>\n",
              "      <td>No mortal is wise at all times</td>\n",
              "      <td>no is  to   little</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>Victoria, Queen and Empress</td>\n",
              "      <td>here and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1559</th>\n",
              "      <td>Promoter of the faith</td>\n",
              "      <td>promoter of the faith</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bring back</td>\n",
              "      <td>report</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>935</th>\n",
              "      <td>In the place of</td>\n",
              "      <td>in the place of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1268</th>\n",
              "      <td>select</td>\n",
              "      <td>pick out</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>O concern</td>\n",
              "      <td>o</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>woman</td>\n",
              "      <td>woman</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>authority, not truth, makes law</td>\n",
              "      <td>the is made   no language</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>849</th>\n",
              "      <td>central</td>\n",
              "      <td>middle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1675</th>\n",
              "      <td>of this</td>\n",
              "      <td>of this</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383</th>\n",
              "      <td>in what manner</td>\n",
              "      <td>how</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>from culture [comes] strength</td>\n",
              "      <td>resist by the other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>548</th>\n",
              "      <td>master</td>\n",
              "      <td>master</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1631</th>\n",
              "      <td>up to</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1305</th>\n",
              "      <td>he found that his troops were hard pressed by ...</td>\n",
              "      <td>and either   to from been and                 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>876</th>\n",
              "      <td>of master</td>\n",
              "      <td>of chief</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>902</th>\n",
              "      <td>As below</td>\n",
              "      <td>as below</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>828</th>\n",
              "      <td>Love conquers all things; let us too surrender...</td>\n",
              "      <td>no is  me</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1743</th>\n",
              "      <td>abode</td>\n",
              "      <td>habitation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1211</th>\n",
              "      <td>For how much longer?</td>\n",
              "      <td>see</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>880</th>\n",
              "      <td>Under God's Spirit she flourishes</td>\n",
              "      <td>the is    the things</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>984</th>\n",
              "      <td>depart</td>\n",
              "      <td>depart</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1392</th>\n",
              "      <td>facing</td>\n",
              "      <td>unfavorable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508</th>\n",
              "      <td>attack</td>\n",
              "      <td>o</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1250</th>\n",
              "      <td>neighboring</td>\n",
              "      <td>be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1408</th>\n",
              "      <td>instrument with voice</td>\n",
              "      <td>from a beginnings</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>step back</td>\n",
              "      <td>recede</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1316</th>\n",
              "      <td>we are born between feces and urine</td>\n",
              "      <td>a   and  not and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1149</th>\n",
              "      <td>facing</td>\n",
              "      <td>turn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>prey</td>\n",
              "      <td>booty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1847</th>\n",
              "      <td>be accustomed</td>\n",
              "      <td>be accustomed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1654</th>\n",
              "      <td>of field</td>\n",
              "      <td>of field land</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1228</th>\n",
              "      <td>grove</td>\n",
              "      <td>grove</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>772</th>\n",
              "      <td>again</td>\n",
              "      <td>back</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1873</th>\n",
              "      <td>ask</td>\n",
              "      <td>ask</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1370</th>\n",
              "      <td>old man</td>\n",
              "      <td>senior</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>574</th>\n",
              "      <td>keep</td>\n",
              "      <td>hold</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>hair</td>\n",
              "      <td>tresses</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>He lives twice who lives well</td>\n",
              "      <td>he who twice who lives</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1435</th>\n",
              "      <td>There they are said to learn a great number of...</td>\n",
              "      <td>the was the  and  orgetorix in your occurs    ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>write</td>\n",
              "      <td>write</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1959</th>\n",
              "      <td>Fools laugh at the Latin language</td>\n",
              "      <td>the is the   language</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>money</td>\n",
              "      <td>money</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 actual                                          predicted\n",
              "523                                     Painter unknown               unknown                             \n",
              "1374                   The following age will be amazed         it age  be amazed                         \n",
              "744                                           set forth                    go                             \n",
              "289                                                robe                    of                             \n",
              "1572                                               fire                of fire                            \n",
              "379                                which was to be done      which was to be done                         \n",
              "464                                                this                  this                             \n",
              "1698                                             of sea               of plain                            \n",
              "426                                            to forms                 forms                             \n",
              "1919                                            chances                     o                             \n",
              "650                                             extreme              situated                             \n",
              "1377                                Wonderful to behold                    to                             \n",
              "630                               The source and origin             the is of in                          \n",
              "684                    False in one thing, false in all         there is  no  in no                       \n",
              "1384     My conscience means more to me than all speech  and is case to  not  than will in  glory      ...\n",
              "1932                                              hopes               to hopes                            \n",
              "717                      No mortal is wise at all times          no is  to   little                       \n",
              "276                         Victoria, Queen and Empress               here and                            \n",
              "1559                              Promoter of the faith    promoter of the faith                          \n",
              "4                                            bring back                report                             \n",
              "935                                     In the place of          in the place of                          \n",
              "1268                                             select               pick out                            \n",
              "202                                           O concern                     o                             \n",
              "243                                               woman                 woman                             \n",
              "1595                    authority, not truth, makes law   the is made   no language                       \n",
              "849                                             central                middle                             \n",
              "1675                                            of this                of this                            \n",
              "383                                      in what manner                   how                             \n",
              "499                       from culture [comes] strength      resist by the other                          \n",
              "548                                              master                master                             \n",
              "1631                                              up to                    to                             \n",
              "1305  he found that his troops were hard pressed by ...  and either   to from been and                 ...\n",
              "876                                           of master               of chief                            \n",
              "902                                            As below               as below                            \n",
              "828   Love conquers all things; let us too surrender...                no is  me                          \n",
              "1743                                              abode            habitation                             \n",
              "1211                               For how much longer?                   see                             \n",
              "880                   Under God's Spirit she flourishes        the is    the things                       \n",
              "984                                              depart                depart                             \n",
              "1392                                             facing           unfavorable                             \n",
              "508                                              attack                     o                             \n",
              "1250                                        neighboring                    be                             \n",
              "1408                              instrument with voice       from a beginnings                           \n",
              "323                                           step back                recede                             \n",
              "1316                we are born between feces and urine            a   and  not and                       \n",
              "1149                                             facing                  turn                             \n",
              "81                                                 prey                 booty                             \n",
              "1847                                      be accustomed          be accustomed                            \n",
              "1654                                           of field           of field land                           \n",
              "1228                                              grove                 grove                             \n",
              "772                                               again                  back                             \n",
              "1873                                                ask                   ask                             \n",
              "1370                                            old man                senior                             \n",
              "574                                                keep                  hold                             \n",
              "86                                                 hair               tresses                             \n",
              "137                       He lives twice who lives well    he who twice who lives                         \n",
              "1435  There they are said to learn a great number of...  the was the  and  orgetorix in your occurs    ...\n",
              "297                                               write                 write                             \n",
              "1959                  Fools laugh at the Latin language      the is the   language                        \n",
              "1999                                              money                 money                             "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFH4LgDojdO0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "e362468e-7b73-44d0-f1de-eec343d973a9"
      },
      "source": [
        "Input_latin = ['longa']\n",
        "source = encode_sequences(lat_tokenizer,lat_length,Input_latin)\n",
        "predict_sequence(model, eng_tokenizer, source)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'long'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIrCOO6D64zt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "32256bba-39eb-4afb-aef0-318c95b65fb8"
      },
      "source": [
        "\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        " \n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        " \n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-latin-both.pkl')\n",
        "train = load_clean_sentences('english-latin-train.pkl')\n",
        "test = load_clean_sentences('english-latin-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "lat_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "lat_vocab_size = len(lat_tokenizer.word_index) + 1\n",
        "lat_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(lat_tokenizer, lat_length, train[:, 1])\n",
        "testX = encode_sequences(lat_tokenizer, lat_length, test[:, 1])\n",
        " \n",
        "# load model\n",
        "model = load_model('Latinmodel2.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, lat_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, lat_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[mīrātus sum], target=[wonder at], predicted=[wonder at]\n",
            "src=[pugnis], target=[battle], predicted=[battle]\n",
            "src=[cursus], target=[of advance], predicted=[o advance]\n",
            "src=[tarda], target=[lingering], predicted=[sluggish]\n",
            "src=[Elizabeth Regina/Eduardus Rex], target=[Queen Elizabeth/King Edward], predicted=[queen elizabeth king edward]\n",
            "src=[ēdere], target=[state], predicted=[state]\n",
            "src=[inimīca], target=[enemy], predicted=[enemy]\n",
            "src=[hospes], target=[stranger], predicted=[guest]\n",
            "src=[semper invicta], target=[always invincible], predicted=[always invincible]\n",
            "src=[pietās], target=[devotion], predicted=[sense]\n",
            "BLEU-1: 0.534008\n",
            "BLEU-2: 0.356906\n",
            "BLEU-3: 0.270156\n",
            "BLEU-4: 0.153844\n",
            "test\n",
            "src=[Nulla regula sine exceptione], target=[There is no rule without exception], predicted=[no is without without without without without]\n",
            "src=[clārus], target=[clear], predicted=[distinguished]\n",
            "src=[quis leget haec?], target=[Who will read this?], predicted=[what who as as]\n",
            "src=[Vita mutatur, non tollitur], target=[Life is changed, not taken away], predicted=[life is is not taken little]\n",
            "src=[referre], target=[bring back], predicted=[report]\n",
            "src=[Ego spem pretio non emo], target=[I do not purchase hope for a price], predicted=[i do not purchase for for a]\n",
            "src=[Felix culpa], target=[Happy fault], predicted=[fortunate fault]\n",
            "src=[Memorabilia], target=[Memorable things], predicted=[memorable things]\n",
            "src=[arbitror], target=[consider], predicted=[think]\n",
            "src=[ex tempore], target=[from [this moment of] time], predicted=[off the cuff without]\n",
            "BLEU-1: 0.291440\n",
            "BLEU-2: 0.164118\n",
            "BLEU-3: 0.116924\n",
            "BLEU-4: 0.052969\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}